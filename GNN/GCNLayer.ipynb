{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Reference\n",
    "\n",
    "- [Basic of Graph Convolution Network](https://www.youtube.com/watch?v=YL1jGgcY78U)\n",
    "- [Molecular Property(logP) Prediction with GCN](https://www.youtube.com/watch?v=htTt4iPJqMg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "- GCN (Graph Convolutional Network) Layer\n",
    "- Skip Connection\n",
    "- GCN Block\n",
    "- Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=None):\n",
    "        super(GCNLayer, self).__init__()\n",
    "\n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "\n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, sc=\"sc\", act=None):\n",
    "        super(GCNBlock, self).__init__\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        act if i != n_layer-1 else None,\n",
    "                                        bn))\n",
    "\n",
    "        if sc == \"sc\":\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc == \"no\":\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "        \n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        identity = x\n",
    "        for i, layer in enumerate(self.layer):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out = self.sc(identity, out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readout(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Readout, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.linear = nn.Linear(self.in_dim, self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.liear.weight)\n",
    "\n",
    "        self.activation = act\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.sc,\n",
    "                                        args.act))\n",
    "        self.readout = Readout(args.hidden_dim,\n",
    "                                args.pred_dim1,\n",
    "                                act=nn.ReLU())\n",
    "        self.pred1 = Predictor(args.pred_dim1,\n",
    "                                args.pred_dim2,\n",
    "                                act=nn.ReLU())\n",
    "        self.pred2 = Predictor(args.pred_dim2,\n",
    "                                args.pred_dim3,\n",
    "                                act=nn.Tanh())\n",
    "        self.pred3 = Predictor(args.pred_dim3,\n",
    "                                args.out_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61e67d4406f83661a218a7594034be74564666d0640d3900a3e99845865d0f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
